{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "if \"src\" not in os.listdir():\n",
    "    os.chdir(\"../../../\")\n",
    "\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.lib.stock_dataset import read_stock_dataset\n",
    "from src.lib.activations import negative_softmax\n",
    "from src.lib.losses import negative_profit_loss, multi_negative_sharpe_ratio_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "symbols = [\"SPY\", \"SPXS\"]\n",
    "n_time_steps = 64\n",
    "batch_size = 200\n",
    "epochs = 3\n",
    "learning_rate = 0.001"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = read_stock_dataset(symbols, \"oc_ret\", n_time_steps)\n",
    "train, valid, test = dataset.train_valid_test_split(timedelta(days=365), timedelta(days=365), scaled=True)\n",
    "n_features = train.n_features\n",
    "n_symbols = train.n_symbols\n",
    "print(f\"Loaded {len(train)} training, {len(valid)}, validation, and {len(test)} testing samples\")\n",
    "print(f\"{n_features} features\")\n",
    "print(f\"{n_symbols} targets\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_model():\n",
    "    target_tensor = Input((n_symbols,))\n",
    "\n",
    "    # LSTM Model\n",
    "    mono_input_tensor = Input((n_time_steps, n_features))\n",
    "    z = mono_input_tensor\n",
    "    lstm_out = LSTM(64)(z)\n",
    "\n",
    "    # Holdings head\n",
    "    z = Dropout(0.25)(lstm_out)\n",
    "    z = Dense(64, activation=\"selu\")(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "    holdings = Dense(n_symbols, activation=negative_softmax, name=\"holdings\")(z)\n",
    "    \n",
    "    # Percent Allocation head\n",
    "    z = Dropout(0.25)(lstm_out)\n",
    "    z = Dense(64, activation=\"selu\")(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "    p_invest = Dense(1, activation=\"sigmoid\", name=\"p_invest\")(z)\n",
    "\n",
    "    # Combine both heads\n",
    "    adj_holdings = holdings * p_invest\n",
    "\n",
    "    # That's the model\n",
    "    mono_model = Model([mono_input_tensor, target_tensor], adj_holdings)\n",
    "\n",
    "    # Apply the sharpe loss to the holdings head\n",
    "    sharpe_loss = multi_negative_sharpe_ratio_loss(target_tensor, holdings)\n",
    "    mono_model.add_loss(sharpe_loss)\n",
    "\n",
    "    # Apply the profit loss to the allocation head\n",
    "    neg_adj_profit = K.sum(negative_profit_loss(target_tensor, adj_holdings), axis=-1)\n",
    "    mono_model.add_loss(neg_adj_profit)\n",
    "    # mono_model.compile(Adam(lr=learning_rate))\n",
    "    \n",
    "    # A model that runs the other model over two samples next to each other in time\n",
    "    bi_feature_tensor = Input((2, n_time_steps, n_features), name=\"bi_features\")\n",
    "    bi_target_tensor = Input((2, n_symbols), name=\"bi_targets\")\n",
    "\n",
    "    X_initial = bi_feature_tensor[:, 0, :, :]\n",
    "    y_initial = bi_target_tensor[:, 0, :]\n",
    "\n",
    "    X_final = bi_feature_tensor[:, 1, :, :]\n",
    "    y_initial = bi_target_tensor[:, 1, :]\n",
    "\n",
    "    holdings_initial = K.stop_gradient(mono_model([X_initial, y_initial]))\n",
    "    holdings_final = mono_model([X_final, y_initial])\n",
    "\n",
    "    # Calculate the squared difference between the two outputs\n",
    "    holding_diff =  0.0001 * K.mean(K.square(holdings_final - holdings_initial), axis=-1)\n",
    "\n",
    "    bi_model = Model([bi_feature_tensor, bi_target_tensor], holding_diff)\n",
    "\n",
    "    bi_model.add_loss(holding_diff)\n",
    "\n",
    "    bi_model.compile(Adam(lr=learning_rate))\n",
    "\n",
    "    # bi_model.summary()\n",
    "\n",
    "    # One more model that simply outputs the desired holdings given an input\n",
    "    pred_model = Model(mono_input_tensor, adj_holdings)\n",
    "\n",
    "    return bi_model, pred_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_batchable_index = train.get_batchable_index(trade_market_open=True)\n",
    "n_iter_per_epoch = len(train_batchable_index)//batch_size\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "model, pred_model = build_model()\n",
    "valid_batchable_index = valid.get_batchable_index(trade_market_open=True)\n",
    "# X_batch_valid, y_batch_valid = valid.get_paired_batch(len(valid_batchable_index)-1, valid_batchable_index, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(range(n_iter_per_epoch))\n",
    "    for i in pbar:\n",
    "\n",
    "        X_batch, y_batch = train.get_paired_batch(batch_size, train_batchable_index, shuffle=True)\n",
    "        loss = model.train_on_batch([X_batch, y_batch])\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        if len(loss_history) > 150:\n",
    "            avg_loss = np.mean(loss_history[-150:])\n",
    "        else:\n",
    "            avg_loss = np.mean(loss_history)\n",
    "        \n",
    "        pbar.set_description(\"loss=%.3f\" % avg_loss)\n",
    "    \n",
    "    # Evaluating the model kills the kernel\n",
    "    # losses_valid = model.evaluate([X_batch_valid, y_batch_valid])\n",
    "    # avg_loss_valid = np.mean(losses_valid)\n",
    "    # print(\"Valid Loss:\", -avg_loss_valid)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_batch_valid_pred, y_batch_valid_pred = valid.get_batch(len(valid_batchable_index), valid_batchable_index, shuffle=False)\n",
    "\n",
    "preds = pred_model.predict(X_batch_valid_pred)\n",
    "for i, ticker in enumerate(symbols):\n",
    "    print(ticker)\n",
    "    plt.hist(preds[:, i], bins=100)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Allocation\")\n",
    "plt.hist(np.abs(preds).sum(-1), bins=100)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = preds[:, 0]\n",
    "y = preds[:, 1]\n",
    "\n",
    "heatmap, xedges, yedges = np.histogram2d(x, y, bins=256)\n",
    "# heatmap = np.log(heatmap+1)\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Heatmap(z=heatmap.T))\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rets = np.exp((np.log(preds * y_batch_valid_pred+1)).sum(axis=-1).cumsum())\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=valid.index, y=rets))\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=valid.index, y=preds[:, 0], name=symbols[0]))\n",
    "fig.add_trace(go.Scatter(x=valid.index, y=preds[:, 1], name=symbols[1]))\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf_arm': conda)"
  },
  "interpreter": {
   "hash": "aad12466045a1575aa1be41a0534cd7508fda1531bcc5a3693dc9ed3be064607"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}